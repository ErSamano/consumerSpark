# consumerSpark

The objective of this project is to emulate a large data pipeline from the source of the data, to the delivery of data after being processed in an ETL process. To then join in a continuous integration and continuous deliveries (CI/CD). The steps to follow in general are:

* Code (IDE)
* GitHub
* Jenkins
*

The idea is to create a consumer to pull data from a Sensor emulating the outcome from IoT (one sensor) and imitate the continuous integration and continuous developer process for the followed pipeline;

1. Data from the IoT generator (Python).
2. Consume that data through spark (Java).
3. The consumer code to Github.
4. Use shippable to make CI/CD.

## Content
1. Source
2. Processing
3. CI/CD
4. Cnclusion

## 1 Source
The first step is the generation of data from a pressure sensor, the idea is to create a consumer after the generate data to pull it from the Sensor emulating the outcome from IoT. The sensor is producing the following parameters:

* Sensor Name	
* Measurement Timestamp	
* Measurement ID
* Resource ID
* Speed Timestamp
* Channel
* Interval Radiation
* Pressure Response Code
* Pressure Type
* Measurement Timestamp Label
* Channel Type ID
* Data Reception Type


### Instructions to execute the the sensor emulator script stand alone:

		python consumerKSensor.py

After run python script it is necessary to run kafka, to do so, execute kafka server (bootstrap server) through the command:

		bin/kafka-server-start.sh config/server-properties

Then the consumer stand alone through the command:

		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic LP300CATs18 --from-beginning

Where **LP300CATs18** is the topic

## 2 Processing

The processing of the data will be done through the Spark tool in version 2.3.1, this tool is so powerful that it allows to perform tasks of Streaming and batch processing.

Will act as a data consumer following some established rules of behavior, such as:

* The data generated by the sensor must have an ID with the following characteristics; The ID was randomly selected in a range between 1 and 5000.
* Based on the tasks elaborated 1 and 2 one of the tests of the code will consist of: Within the ID of each data, if an ID will be repeated more than 5 times then it is considered a failure.
* Within the three time data must be added +/- (plus or minus) 5 minutes to the times to have different times in the same data entry.
* The spark code should work as a window, this to exemplify the waiting times of the data within spark

## 3 CI/CD

Complete the general process map for a full CI/CD through Jenkins

* Implement the Jenkins tool with at least one test for the use of QA

## 4 Cnclusion

